{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gr7HfhEl2Bxu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/DIEBACK-20250409T065230Z-001.zip'\n",
        "extract_dir = '/content/DIEBACK'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Optional: Clean nested folders if needed\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    for d in dirs:\n",
        "        print(os.path.join(root, d))  # Check subfolder structure\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l6VSnlq2ujB",
        "outputId": "e10f95f0-77ae-44f5-fa15-dd4024b56e6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DIEBACK/DIEBACK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "image_size = 64\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),  # Converts to [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root=extract_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "P7Hz7cWQ3Hgr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, 2, 1),  # 64x64 → 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),  # 32x32 → 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),  # 16x16 → 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128 * 8 * 8, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128 * 8 * 8, latent_dim)\n",
        "        self.fc_decode = nn.Linear(latent_dim, 128 * 8 * 8)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (128, 8, 8)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 8x8 → 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 16x16 → 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1),  # 32x32 → 64x64\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_encoded = self.encoder(x)\n",
        "        mu = self.fc_mu(x_encoded)\n",
        "        logvar = self.fc_logvar(x_encoded)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        decoded = self.fc_decode(z)\n",
        "        decoded = self.decoder(decoded)\n",
        "        return decoded, mu, logvar\n"
      ],
      "metadata": {
        "id": "rv-hhmQ53OUu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vae = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "def vae_loss(x_recon, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_div\n",
        "\n",
        "epochs = 20\n",
        "vae.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for images, _ in dataloader:\n",
        "        images = images.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed, mu, logvar = vae(images)\n",
        "        loss = vae_loss(reconstructed, images, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader.dataset):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DpskGS_3Vp7",
        "outputId": "cd965b4d-d9e1-47ed-f60a-2f655d1cd6e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2565.04\n",
            "Epoch 2/20, Loss: 2488.26\n",
            "Epoch 3/20, Loss: 2413.42\n",
            "Epoch 4/20, Loss: 2347.48\n",
            "Epoch 5/20, Loss: 2283.98\n",
            "Epoch 6/20, Loss: 2273.17\n",
            "Epoch 7/20, Loss: 2175.17\n",
            "Epoch 8/20, Loss: 2139.04\n",
            "Epoch 9/20, Loss: 2113.99\n",
            "Epoch 10/20, Loss: 2071.60\n",
            "Epoch 11/20, Loss: 2063.54\n",
            "Epoch 12/20, Loss: 2014.10\n",
            "Epoch 13/20, Loss: 1981.59\n",
            "Epoch 14/20, Loss: 1935.85\n",
            "Epoch 15/20, Loss: 1892.45\n",
            "Epoch 16/20, Loss: 1874.65\n",
            "Epoch 17/20, Loss: 1815.12\n",
            "Epoch 18/20, Loss: 1781.75\n",
            "Epoch 19/20, Loss: 1740.12\n",
            "Epoch 20/20, Loss: 1691.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and save 100+ images from trained VAE\n",
        "import torchvision.utils as vutils\n",
        "import os\n",
        "\n",
        "vae.eval()\n",
        "vae_folder = \"/content/generated_vae\"\n",
        "os.makedirs(vae_folder, exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(100):\n",
        "        z = torch.randn(1, 128).to(device)\n",
        "        sample = vae.decoder(vae.fc_decode(z)).cpu()\n",
        "        vutils.save_image(sample, f\"{vae_folder}/vae_sample_{i}.png\", normalize=True)\n"
      ],
      "metadata": {
        "id": "0OUsxhv1e3LA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Generator and Discriminator for DCGAN\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz=100, ngf=64, nc=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.net(input)\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),  # (64 x 32 x 32)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False), # (128 x 16 x 16)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False), # (256 x 8 x 8)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False), # (512 x 4 x 4)\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),  # (1 x 1 x 1)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1)  # Flatten to (batch_size,)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4qUrvK1hhQc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train DCGAN\n",
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "nz = 100\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        real_images = real_images.to(device)\n",
        "        b_size = real_images.size(0) # Update b_size with the actual batch size\n",
        "\n",
        "        # Labels - Recreate labels with the correct batch size\n",
        "        real_labels = torch.full((b_size,), 1., device=device)\n",
        "        fake_labels = torch.full((b_size,), 0., device=device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        netD.zero_grad()\n",
        "        output_real = netD(real_images)\n",
        "\n",
        "        # Reshape output_real to match real_labels\n",
        "        output_real = output_real.view(-1) # flatten the output to have b_size elements\n",
        "\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake_images = netG(noise)\n",
        "        output_fake = netD(fake_images.detach())\n",
        "\n",
        "        # Reshape output_fake to match fake_labels\n",
        "        output_fake = output_fake.view(-1) # flatten the output to have b_size elements\n",
        "\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        lossD = loss_real + loss_fake\n",
        "        lossD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Train Generator\n",
        "        netG.zero_grad()\n",
        "        output = netD(fake_images)\n",
        "        # Reshape output to match real_labels\n",
        "        output = output.view(-1) # flatten the output to have b_size elements\n",
        "        lossG = criterion(output, real_labels)\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss_D: {lossD.item():.4f} | Loss_G: {lossG.item():.4f}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQiVMecdhuc3",
        "outputId": "a3d9441c-5733-49af-faeb-c5662fefa2dc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Loss_D: 1.3286 | Loss_G: 2.1037\n",
            "Epoch 2/20 | Loss_D: 1.2455 | Loss_G: 3.3764\n",
            "Epoch 3/20 | Loss_D: 0.5610 | Loss_G: 4.4465\n",
            "Epoch 4/20 | Loss_D: 0.2919 | Loss_G: 4.5590\n",
            "Epoch 5/20 | Loss_D: 0.2490 | Loss_G: 4.7280\n",
            "Epoch 6/20 | Loss_D: 0.2296 | Loss_G: 5.3555\n",
            "Epoch 7/20 | Loss_D: 0.1747 | Loss_G: 5.5980\n",
            "Epoch 8/20 | Loss_D: 0.1372 | Loss_G: 5.7143\n",
            "Epoch 9/20 | Loss_D: 0.1425 | Loss_G: 6.0916\n",
            "Epoch 10/20 | Loss_D: 0.0932 | Loss_G: 6.1360\n",
            "Epoch 11/20 | Loss_D: 0.1016 | Loss_G: 6.4213\n",
            "Epoch 12/20 | Loss_D: 0.0963 | Loss_G: 6.7892\n",
            "Epoch 13/20 | Loss_D: 0.1004 | Loss_G: 7.0395\n",
            "Epoch 14/20 | Loss_D: 0.0790 | Loss_G: 6.8067\n",
            "Epoch 15/20 | Loss_D: 0.1005 | Loss_G: 7.3989\n",
            "Epoch 16/20 | Loss_D: 0.0669 | Loss_G: 7.1173\n",
            "Epoch 17/20 | Loss_D: 0.0665 | Loss_G: 7.0737\n",
            "Epoch 18/20 | Loss_D: 0.0660 | Loss_G: 7.5784\n",
            "Epoch 19/20 | Loss_D: 0.0474 | Loss_G: 7.2551\n",
            "Epoch 20/20 | Loss_D: 0.0614 | Loss_G: 7.8317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 100+ synthetic GAN images\n",
        "gan_folder = \"/content/generated_gan\"\n",
        "os.makedirs(gan_folder, exist_ok=True)\n",
        "\n",
        "netG.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(100):\n",
        "        noise = torch.randn(1, nz, 1, 1, device=device)\n",
        "        fake_img = netG(noise).cpu()\n",
        "        vutils.save_image(fake_img, f\"{gan_folder}/gan_sample_{i}.png\", normalize=True)\n"
      ],
      "metadata": {
        "id": "ao_BprJNlaBH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # Change to 'cpu'"
      ],
      "metadata": {
        "id": "F0VeopdtmYMc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "import os\n",
        "!pip install pytorch-fid\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import cleanfid as fid\n",
        "from cleanfid import fid\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Compute FID\n",
        "fid_vae = fid.compute_fid(\"/content/DIEBACK/DIEBACK\", vae_folder, device=device)\n",
        "fid_gan = fid.compute_fid(\"/content/DIEBACK/DIEBACK\", gan_folder, device=device)\n",
        "print(\"FID (VAE):\", fid_vae)\n",
        "print(\"FID (GAN):\", fid_gan)\n",
        "\n",
        "# Compute SSIM\n",
        "def calculate_ssim_folder(real_dir, generated_dir):\n",
        "    real_imgs = sorted(os.listdir(real_dir))[:100]\n",
        "    gen_imgs = sorted(os.listdir(generated_dir))[:100]\n",
        "\n",
        "    total_ssim = 0\n",
        "    for real_name, gen_name in zip(real_imgs, gen_imgs):\n",
        "        real = np.array(Image.open(os.path.join(real_dir, real_name)).convert(\"L\").resize((64,64)))\n",
        "        gen = np.array(Image.open(os.path.join(generated_dir, gen_name)).convert(\"L\").resize((64,64)))\n",
        "        ssim_val = ssim(real, gen, data_range=255)\n",
        "        total_ssim += ssim_val\n",
        "\n",
        "    return total_ssim / len(real_imgs)\n",
        "\n",
        "ssim_vae = calculate_ssim_folder(\"/content/DIEBACK/DIEBACK\", vae_folder)\n",
        "ssim_gan = calculate_ssim_folder(\"/content/DIEBACK/DIEBACK\", gan_folder)\n",
        "print(\"SSIM (VAE):\", ssim_vae)\n",
        "print(\"SSIM (GAN):\", ssim_gan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJcZf4Volc0R",
        "outputId": "f21aca20-25d1-484e-ae39-f6bbe1f7809b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (11.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute FID between two folders\n",
            "Found 47 images in the folder /content/DIEBACK/DIEBACK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FID DIEBACK : 100%|██████████| 2/2 [00:19<00:00,  9.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 images in the folder /content/generated_vae\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FID generated_vae : 100%|██████████| 4/4 [00:38<00:00,  9.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute FID between two folders\n",
            "Found 47 images in the folder /content/DIEBACK/DIEBACK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FID DIEBACK : 100%|██████████| 2/2 [00:18<00:00,  9.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 images in the folder /content/generated_gan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FID generated_gan : 100%|██████████| 4/4 [00:40<00:00, 10.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID (VAE): 415.69851736698956\n",
            "FID (GAN): 512.7580129378562\n",
            "SSIM (VAE): 0.10447535899447119\n",
            "SSIM (GAN): 0.03678041522274954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JqJDlDhglh6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}